{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4297245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriele/miniconda3/envs/galaxy_project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gabriele/miniconda3/envs/galaxy_project/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647352509/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#################INSTALL iopath, fvcore, optuna diocanegg###########\n",
    "### Cose da fare: \n",
    "# impostare il resize bene, per ora è a 28.\n",
    "#  Impostare is_rgb per avere 1 o 3 canali. \n",
    "# Impostare una evaluation corretta\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3de35",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Fine tuning dei parametri ---> optuna\n",
    "- Capire come salvare i dati [(documentazione)](https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html) FATTO credo\n",
    "- mapping delle probabilità da aggiungere alla rete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d01e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyJungle(Dataset): # sarebbe interessante implementare un rescale/crop\n",
    "    \n",
    "    #the init function initializes the directory containing the image,\n",
    "    #the annotations file,\n",
    "    #and both transforms\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None, is_rgb=False):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.is_rgb = is_rgb\n",
    "\n",
    "    #returns number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    #loads a sample from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[idx, 0])) + '.jpg'\n",
    "        #retrieves the image\n",
    "        image = Image.open(img_path)\n",
    "        if not self.is_rgb: image = image.convert('L')\n",
    "        #retrieves corresponding label\n",
    "        label = self.img_labels.iloc[idx, 1:]\n",
    "        #if possible, transform the image and the label into a tensor.\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(label.values, dtype=torch.float32)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label#, self.img_labels.iloc[idx, 0]\n",
    "    \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), #fa già la normalizzazione se l'immagine non è un tensore\n",
    "    # sarebbe interessante implementare un random crop prima del center crop per decentrare un poco le immagini????\n",
    "    transforms.RandomHorizontalFlip(), # horizontal flip\n",
    "    transforms.RandomVerticalFlip(), \n",
    "    transforms.Resize((28,28))# vertical flip\n",
    "    #transforms.CenterCrop(csize)          #CROP\n",
    "    ]) #transforms.compose per fare una pipe di transformazioni\n",
    "\n",
    "\n",
    "\n",
    "DS = GalaxyJungle('../data/training/training_solutions_rev1.csv', '../data/training/', transform=transform,is_rgb=False)\n",
    "training, test_set = random_split(DS, [.8, .2])\n",
    "#training[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5513118",
   "metadata": {},
   "source": [
    "img, lab, indx = DS.__getitem__(0)\n",
    "#print(lab)\n",
    "#print(img)         #3D TENSOR    \n",
    "if is_rgb:\n",
    "    fig, ax = plt.subplots(1,3, figsize=(24,7))\n",
    "    color = ['Reds', 'Greens', 'Blues']\n",
    "    for i,j in enumerate(img):\n",
    "        ax[i].imshow(j, cmap=color[i])\n",
    "else:\n",
    "    fig, ax = plt.subplots(1,1, figsize=(24,7))\n",
    "    ax.imshow(img[0], cmap='magma')\n",
    "#print(img.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c51673",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0de85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyNet(nn.Module):\n",
    "    def __init__(self, trial, num_conv_layers, num_filters, num_neurons, drop_conv2, drop_fc1):\n",
    "        \n",
    "        super(GalaxyNet, self).__init__()\n",
    "        input_size = 28\n",
    "        kernel_size = 3\n",
    "        output_size=input_size\n",
    "        padding = 0\n",
    "        dilation = 1\n",
    "        stride = 2\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters[0], kernel_size=(3,3))])\n",
    "        output_size = output_size - kernel_size + 1\n",
    "        output_size = output_size // 2\n",
    "        #output_size = int(((input_size + 2 * padding - dilation * (kernel_size -1)) / stride) + 1 )\n",
    "        for i in range(1, num_conv_layers):\n",
    "            self.convs.append(nn.Conv2d(in_channels=num_filters[i-1], out_channels=num_filters[i], kernel_size=(3,3)))\n",
    "            output_size = output_size - kernel_size + 1\n",
    "            output_size = output_size // 2\n",
    "            #output_size = int(((input_size + 2 * padding - dilation * (kernel_size -1)) / stride) + 1 )\n",
    "        self.conv2_drop = nn.Dropout2d(p=drop_conv2)\n",
    "        self.out_feature = num_filters[num_conv_layers-1] * output_size * output_size\n",
    "        self.fc1 = nn.Linear(self.out_feature, num_neurons) # fc è fully connected, #120 neuroni che prendono l'output\n",
    "        self.fc2 = nn.Linear(num_neurons, 37)# un altro fc layer che prende dai 120- neuroni e connette a 84 neuroni\n",
    "        self.p1 = drop_fc1\n",
    "        #i numeri non vincolati sono il primo 6 e il primo 16 e poi i numeri di neuroni\n",
    "        ## Instance variables:\n",
    "        for i in range(1, num_conv_layers):\n",
    "            nn.init.kaiming_normal_(self.convs[i].weight, nonlinearity='relu')\n",
    "            if self.convs[i].bias is not None:\n",
    "                nn.init.constant_(self.convs[i].bias,0)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv_i in enumerate(self.convs):  # For each convolutional layer\n",
    "            if i == 2:  # Add dropout if layer 2\n",
    "                x = F.relu(F.max_pool2d(self.conv2_drop(conv_i(x)), 2))  # Conv_i, dropout, max-pooling, RelU\n",
    "            else:\n",
    "                x = F.relu(F.max_pool2d(conv_i(x), 2))                   # Conv_i, max-pooling, RelU\n",
    "\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #x = x.flatten()                    # Flatten tensor\n",
    "        x = F.relu(self.fc1(x))                              # FC1, RelU\n",
    "        x = F.dropout(x, p=self.p1, training=self.training)  # Apply dropout after FC1 only when training\n",
    "        x = self.fc2(x)    \n",
    "        return x\n",
    "    \n",
    "    #def convool_size(input_dimension, padding, dilation, kernel_size, stride):\n",
    "        '''\n",
    "        Returns Conv2d and Max_pool output size for a square input tensor.\n",
    "        '''\n",
    "        return ((input_dimension + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1\n",
    "    \n",
    "    #def log_the_loss(self, item,epoch=False, validation=False): # per avere una history della loss???\n",
    "   #     if epoch and not validation:\n",
    "   #         self.loss_dict['epoch'].append(item)\n",
    "   #     elif not epoch and not validation:\n",
    "   #         self.loss_dict['batch'].append(item)\n",
    "   #     elif validation and epoch:\n",
    "   #         self.loss_dict['vepoch'].append(item)\n",
    "   #     elif validation and not epoch:\n",
    "   #         self.loss_dict['vbatch'].append(item)\n",
    "   #     return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f5f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, optimizer):\n",
    "    network.train()\n",
    "    for batch_index, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data.to(device))\n",
    "        loss = F.mse_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(network):\n",
    "    \n",
    "    network.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for  data,target in test_loader:\n",
    "            output = network(data.to(device))\n",
    "            loss = F.mse_loss(output, target.to(device), reduction='sum')\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss /len(test_loader.dataset)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2287e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 2, 3)\n",
    "    num_filters = [int(trial.suggest_discrete_uniform(\"num_filters_\"+str(i), 16, 128, 16)) for i in range(num_conv_layers)]\n",
    "    num_neurons = trial.suggest_int(\"num_neurons\",10,400,10)\n",
    "    drop_conv2 = trial.suggest_float(\"drop_conv2\", 0.2, 0.5)\n",
    "    drop_fc1 = trial.suggest_float(\"drop_fc1\", 0.2,0.5)\n",
    "\n",
    "    model = GalaxyNet(trial, num_conv_layers, num_filters, num_neurons, drop_conv2, drop_fc1).to(device)\n",
    "    #model = GalaxyNet(trial, 2, num_filters, 10, 0.1, 0.1).to(device)\n",
    "    \n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        train(model,optimizer)\n",
    "        loss = test(model)\n",
    "        trial.report(loss, epoch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0804377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 12:29:47,028] A new study created in memory with name: no-name-649e4dca-5059-42eb-8b1d-8a3a394985d2\n",
      "/tmp/ipykernel_34479/478379788.py:3: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  num_filters = [int(trial.suggest_discrete_uniform(\"num_filters_\"+str(i), 16, 128, 16)) for i in range(num_conv_layers)]\n",
      "/tmp/ipykernel_34479/478379788.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_neurons = trial.suggest_int(\"num_neurons\",10,400,10)\n",
      "[I 2025-05-15 12:30:17,172] Trial 0 finished with value: 1.0023457522901442 and parameters: {'num_conv_layers': 2, 'num_filters_0': 48.0, 'num_filters_1': 96.0, 'num_neurons': 260, 'drop_conv2': 0.279551308543565, 'drop_fc1': 0.23874116706577286, 'optimizer': 'Adam', 'lr': 0.062050700999970086}. Best is trial 0 with value: 1.0023457522901442.\n",
      "[I 2025-05-15 12:30:54,058] Trial 1 finished with value: 0.6902045333187027 and parameters: {'num_conv_layers': 3, 'num_filters_0': 48.0, 'num_filters_1': 80.0, 'num_filters_2': 112.0, 'num_neurons': 250, 'drop_conv2': 0.253777215190201, 'drop_fc1': 0.31122036582401325, 'optimizer': 'Adam', 'lr': 0.0006184679750972584}. Best is trial 1 with value: 0.6902045333187027.\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "nepochs = 1                         # Number of training epochs\n",
    "batch_size_train = 64                # Batch size for training data\n",
    "batch_size_test = 420               # Batch size for testing data\n",
    "number_of_trials = 2                # Number of Optuna trials\n",
    "\n",
    "train_loader = DataLoader(training,batch_size=batch_size_train, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size_test, shuffle=True, num_workers=8)\n",
    "\n",
    "study = optuna.create_study(direction='minimize') \n",
    "study.optimize(objective, n_trials=number_of_trials)\n",
    "best_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead02e1",
   "metadata": {},
   "source": [
    "## Traininig + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04322d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_train(verbose=False):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    gnet.train(True)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels, idx = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = gnet(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        RMSEloss =  np.sqrt(loss.item())\n",
    "        if verbose and i%10==0: print(f'Batch {i+1}/{len(train_loader)} - Loss: {RMSEloss:.3f}')\n",
    "        running_loss += RMSEloss\n",
    "        gnet.log_the_loss(RMSEloss, epoch=False, validation=False)\n",
    "        if i == len(train_loader)-1:\n",
    "            epochmean_loss = running_loss / len(train_loader)\n",
    "            if verbose: print(f\"---\\nEpoch {epoch} - Loss: {epochmean_loss:.3f}\\n---\")\n",
    "            gnet.log_the_loss(epochmean_loss, epoch=True, validation=False)\n",
    "            last_loss = RMSEloss\n",
    "            if verbose: print(f\"---\\nEpoch {epoch} - Loss: {last_loss:.3f}\\n---\")\n",
    "    return last_loss\n",
    "\n",
    "def one_epoch_eval(verbose=False):\n",
    "    gnet.eval()\n",
    "    running_validation_loss = 0.\n",
    "    if verbose: print('Evaluation...')\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_loader):\n",
    "            inputs, labels, idx = vdata\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = gnet(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            RMSEloss =  np.sqrt(loss.item())\n",
    "            running_validation_loss += RMSEloss\n",
    "            gnet.log_the_loss(RMSEloss,epoch=False, validation = True)\n",
    "    mean_vloss = gnet.log_the_loss(RMSEloss/(i+1), epoch=True, validation=True)\n",
    "    if verbose: print(f\"---\\nValidation Loss: {mean_vloss:.3f}\\n---\")\n",
    "    return mean_vloss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f70d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "Batch 1/385 - Loss: 0.275\n",
      "Batch 11/385 - Loss: 0.283\n",
      "Batch 21/385 - Loss: 0.266\n",
      "Batch 31/385 - Loss: 0.276\n",
      "Batch 41/385 - Loss: 0.264\n",
      "Batch 51/385 - Loss: 0.266\n",
      "Batch 61/385 - Loss: 0.255\n",
      "Batch 71/385 - Loss: 0.255\n",
      "Batch 81/385 - Loss: 0.247\n",
      "Batch 91/385 - Loss: 0.247\n",
      "Batch 101/385 - Loss: 0.244\n",
      "Batch 111/385 - Loss: 0.242\n",
      "Batch 121/385 - Loss: 0.235\n",
      "Batch 131/385 - Loss: 0.226\n",
      "Batch 141/385 - Loss: 0.227\n",
      "Batch 151/385 - Loss: 0.231\n",
      "Batch 161/385 - Loss: 0.212\n",
      "Batch 171/385 - Loss: 0.213\n",
      "Batch 181/385 - Loss: 0.210\n",
      "Batch 191/385 - Loss: 0.214\n",
      "Batch 201/385 - Loss: 0.201\n",
      "Batch 211/385 - Loss: 0.193\n",
      "Batch 221/385 - Loss: 0.188\n",
      "Batch 231/385 - Loss: 0.186\n",
      "Batch 241/385 - Loss: 0.187\n",
      "Batch 251/385 - Loss: 0.177\n",
      "Batch 261/385 - Loss: 0.178\n",
      "Batch 271/385 - Loss: 0.180\n",
      "Batch 281/385 - Loss: 0.178\n",
      "Batch 291/385 - Loss: 0.176\n",
      "Batch 301/385 - Loss: 0.172\n",
      "Batch 311/385 - Loss: 0.157\n",
      "Batch 321/385 - Loss: 0.184\n",
      "Batch 331/385 - Loss: 0.162\n",
      "Batch 341/385 - Loss: 0.171\n",
      "Batch 351/385 - Loss: 0.160\n",
      "Batch 361/385 - Loss: 0.171\n",
      "Batch 371/385 - Loss: 0.163\n",
      "Batch 381/385 - Loss: 0.163\n",
      "---\n",
      "Epoch 0 - Loss: 0.211\n",
      "---\n",
      "---\n",
      "Epoch 0 - Loss: 0.170\n",
      "---\n",
      "Evaluation...\n",
      "---\n",
      "Validation Loss: 0.002\n",
      "---\n",
      "Training epoch 1\n",
      "Batch 1/385 - Loss: 0.173\n",
      "Batch 11/385 - Loss: 0.167\n",
      "Batch 21/385 - Loss: 0.164\n",
      "Batch 31/385 - Loss: 0.174\n",
      "Batch 41/385 - Loss: 0.162\n",
      "Batch 51/385 - Loss: 0.169\n",
      "Batch 61/385 - Loss: 0.162\n",
      "Batch 71/385 - Loss: 0.170\n",
      "Batch 81/385 - Loss: 0.167\n",
      "Batch 91/385 - Loss: 0.162\n",
      "Batch 101/385 - Loss: 0.167\n",
      "Batch 111/385 - Loss: 0.165\n",
      "Batch 121/385 - Loss: 0.159\n",
      "Batch 131/385 - Loss: 0.168\n",
      "Batch 141/385 - Loss: 0.162\n",
      "Batch 151/385 - Loss: 0.171\n",
      "Batch 161/385 - Loss: 0.163\n",
      "Batch 171/385 - Loss: 0.167\n",
      "Batch 181/385 - Loss: 0.162\n",
      "Batch 191/385 - Loss: 0.171\n",
      "Batch 201/385 - Loss: 0.168\n",
      "Batch 211/385 - Loss: 0.161\n",
      "Batch 221/385 - Loss: 0.164\n",
      "Batch 231/385 - Loss: 0.164\n",
      "Batch 241/385 - Loss: 0.162\n",
      "Batch 251/385 - Loss: 0.164\n",
      "Batch 261/385 - Loss: 0.170\n",
      "Batch 271/385 - Loss: 0.164\n",
      "Batch 281/385 - Loss: 0.157\n",
      "Batch 291/385 - Loss: 0.167\n",
      "Batch 301/385 - Loss: 0.164\n",
      "Batch 311/385 - Loss: 0.154\n",
      "Batch 321/385 - Loss: 0.167\n",
      "Batch 331/385 - Loss: 0.164\n",
      "Batch 341/385 - Loss: 0.161\n",
      "Batch 351/385 - Loss: 0.160\n",
      "Batch 361/385 - Loss: 0.157\n",
      "Batch 371/385 - Loss: 0.148\n",
      "Batch 381/385 - Loss: 0.165\n",
      "---\n",
      "Epoch 1 - Loss: 0.165\n",
      "---\n",
      "---\n",
      "Epoch 1 - Loss: 0.159\n",
      "---\n",
      "Evaluation...\n",
      "---\n",
      "Validation Loss: 0.002\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,2):\n",
    "    print(f'Training epoch {epoch}')\n",
    "    epoch_last_loss = one_epoch_train(verbose=True)\n",
    "    #print(f'Epoch {epoch} - Loss: {epoch_last_loss:.3f}')\n",
    "    epoch_vloss = one_epoch_eval(verbose=True)\n",
    "    #print(f'Epoch {epoch} - Validation Loss: {epoch_vloss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d7d13",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore our model parameter so we don't have to retrain if we want to use it again\n",
    "now = datetime.now().strftime(\"%d-%m_%H-%M-%S\")\n",
    "model_last_name = f'gnet/trained_gnet_{now}.pth'\n",
    "torch.save({\n",
    "    'model_state_dict' : gnet.state_dict(), # tutti i pesi\n",
    "    'optimizer_state_dict' : optimizer.state_dict(), # values of the optimizer, loss is just the last loss value.\n",
    "    'loss' : gnet.loss_dict,\n",
    "    'batchsize_train_test' : (batch_size_train, batch_size_test),\n",
    "    'device' : device,\n",
    "    'is_rgb' : is_rgb,\n",
    "    'crop_size' : csize,\n",
    "    'kernel_size' : kernel_size,\n",
    "    'out_channels' : out_channels,\n",
    "    'feature_map_2' : feature_map_2,\n",
    "    'max_pool_kernel' : max_pool_kernel,\n",
    "    'loss_function' : str(loss_function),\n",
    "    'optimizer' : str(optimizer),\n",
    "    'epoch' : len(gnet.loss_dict['epoch'])}, model_last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d0ef3",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce07130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26239/2436948299.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  gnet.load_state_dict(torch.load(model_last_name)['model_state_dict'])\n",
      "/tmp/ipykernel_26239/2436948299.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(model_last_name)['optimizer_state_dict'])\n",
      "/tmp/ipykernel_26239/2436948299.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loss = torch.load(model_last_name)['loss']\n"
     ]
    }
   ],
   "source": [
    "gnet = GalaxyNet()\n",
    "gnet.load_state_dict(torch.load(model_last_name)['model_state_dict'])\n",
    "optimizer.load_state_dict(torch.load(model_last_name)['optimizer_state_dict'])\n",
    "loss = torch.load(model_last_name)['loss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galaxy_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
