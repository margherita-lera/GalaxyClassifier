{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fd1b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import zookeeper as zk\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb45ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyJungle(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, mappy=False, is_rgb=False):\n",
    "        self.rgb = is_rgb\n",
    "        self.mappy = mappy\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.img_labels).shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[idx, 0])) + '.jpg'\n",
    "        #retrieves the image\n",
    "        image = Image.open(img_path)\n",
    "        if not self.rgb: image = image.convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)#.type(torch.float16)\n",
    "        #retrieves corresponding label\n",
    "        label = self.img_labels.iloc[idx, 1:]        \n",
    "        label = torch.tensor(label.values, dtype=torch.float32)\n",
    "        if self.mappy: label = zk.mappy(label)\n",
    "\n",
    "        return image, label, self.img_labels.iloc[idx, 0]\n",
    "    \n",
    "\n",
    "transfs = transforms.Compose([\n",
    "    transforms.ToTensor(), # Riscala le immagini tra 0 e 1\n",
    "    transforms.CenterCrop(324),\n",
    "    transforms.Resize(128)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0317daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = GalaxyJungle('../data/test/test_solutions_rev1.csv', '../data/test/', transfs, mappy=True, is_rgb=False)\n",
    "test_loader = DataLoader(DS, batch_size=32, shuffle=False, num_workers=os.cpu_count())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyNet(nn.Module):\n",
    "    def __init__(self, activation, initialization=False, mappy=False, is_rgb=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mappy = mappy\n",
    "        rgb = 3 if is_rgb else 1\n",
    "        input_size = 128\n",
    "        num_labels = 37\n",
    "        \n",
    "        self.loss_dict = {'batch' : [], 'epoch' : [], 'vbatch' : [], 'vepoch' : []}\n",
    "        self.activation = activation\n",
    "\n",
    "        \n",
    "        ## convolutional layers\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(rgb, 16, 3, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            self.activation(),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(16, 16, 3, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            self.activation(),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            self.activation(),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            self.activation(),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, bias=False),\n",
    "            nn.BatchNorm2d(128),            \n",
    "            self.activation(),\n",
    "\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "        for layer in self.convs:\n",
    "            if layer.__class__.__name__ == 'Conv2d': input_size = zk.convool_size(input_size, 3, 1, 'same' if layer.padding == 'same' else 0)\n",
    "            elif layer.__class__.__name__ == 'MaxPool2d': input_size = zk.convool_size(input_size, 2, 2)\n",
    "\n",
    "        if input_size < 2: raise ValueError('You shrank too much dude.')\n",
    "        print(f'Convs output size: {input_size}')\n",
    "\n",
    "        input_linear = 128 * input_size * input_size\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_linear, 100),\n",
    "            self.activation(),\n",
    "            nn.Linear(100, num_labels)\n",
    "            )\n",
    "\n",
    "        if initialization: self.init_weights()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.fc(x)\n",
    "        if self.mappy: x = zk.mappy2D(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        if self.activation == nn.ReLU:\n",
    "            nonlin = 'relu'\n",
    "            a = 0\n",
    "        elif self.activation == nn.LeakyReLU:\n",
    "            nonlin = 'leaky_relu'\n",
    "            a = .01\n",
    "        \n",
    "        # Init convolutional parameters\n",
    "        for layer in self.convs: \n",
    "            if layer.__class__.__name__ == 'Conv2d': nn.init.kaiming_normal_(layer.weight, a=a, nonlinearity=nonlin)\n",
    "        \n",
    "\n",
    "        # Init linear parameters\n",
    "        for i in (1, -1): nn.init.constant_(self.fc[i].bias, 0)\n",
    "        nn.init.kaiming_normal_(self.fc[1].weight, a=a, nonlinearity=nonlin)\n",
    "        nn.init.xavier_uniform_(self.fc[-1].weight)      \n",
    "        \n",
    "\n",
    "\n",
    "    def log_the_loss(self, item,epoch=False): # per avere una history della loss???\n",
    "        verbose=False\n",
    "        train = self.__getstate__()['training']\n",
    "        if verbose: print(train)\n",
    "        if epoch and train:\n",
    "            self.loss_dict['epoch'].append(item) ### get state of the model so you can ditch the validation parameter\n",
    "        elif not epoch and train:\n",
    "            self.loss_dict['batch'].append(item)\n",
    "        elif not train and epoch:\n",
    "            self.loss_dict['vepoch'].append(item)\n",
    "        elif not train and not epoch:\n",
    "            self.loss_dict['vbatch'].append(item)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7e088aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convs output size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "193it [00:08, 23.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.06178793097018647\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = GalaxyNet(nn.ReLU, mappy=True).to(device)\n",
    "loader = torch.load('/home/teobaldo/Uni/LCP-B/proj/Training/INVJAGZooNet/191_model/model_190.pt', weights_only=True)\n",
    "model.load_state_dict(loader)\n",
    "model.eval()\n",
    "running_validation_loss = 0\n",
    "with torch.no_grad(): # deactivates gradient evaluation\n",
    "        for i, vdata in tqdm(enumerate(test_loader)):\n",
    "            inputs,labels, _ = vdata\n",
    "            inputs,labels= inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)#, activation=F.relu)\n",
    "            loss = nn.MSELoss()(outputs ,labels)\n",
    "            RMSEloss = np.sqrt(loss.item())\n",
    "            running_validation_loss += RMSEloss\n",
    "            model.log_the_loss(RMSEloss,epoch=False)\n",
    "        mean_vloss=model.log_the_loss(running_validation_loss / len(test_loader),epoch=True)\n",
    "        print(f\"Test Loss: {mean_vloss}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cb3391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyNet(nn.Module):\n",
    "    def __init__(self, activation, initialization=False, mappy=False, is_rgb=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mappy = mappy\n",
    "        rgb = 3 if is_rgb else 1\n",
    "        input_size = 128\n",
    "        num_labels = 37\n",
    "        \n",
    "        self.loss_dict = {'batch' : [], 'epoch' : [], 'vbatch' : [], 'vepoch' : []}\n",
    "        self.activation = activation\n",
    "\n",
    "        \n",
    "        ## convolutional layers\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(rgb, 16, 3, padding='same', bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(16),\n",
    "\n",
    "            nn.Conv2d(16, 16, 3, padding='same', bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding='same', bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 32, 3, padding='same', bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "        for layer in self.convs:\n",
    "            if layer.__class__.__name__ == 'Conv2d': input_size = zk.convool_size(input_size, 3, 1, 'same' if layer.padding == 'same' else 0)\n",
    "            elif layer.__class__.__name__ == 'MaxPool2d': input_size = zk.convool_size(input_size, 2, 2)\n",
    "\n",
    "        if input_size < 2: raise ValueError('You shrank too much dude.')\n",
    "        print(f'Convs output size: {input_size}')\n",
    "\n",
    "        input_linear = 256 * input_size * input_size\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_linear, 100),\n",
    "            self.activation(),\n",
    "            nn.Linear(100, num_labels)\n",
    "            )\n",
    "        \n",
    "        if initialization: self.init_weights()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.fc(x)\n",
    "        if self.mappy: x = zk.mappy2D(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        if self.activation == nn.ReLU:\n",
    "            nonlin = 'relu'\n",
    "            a = 0\n",
    "        elif self.activation == nn.LeakyReLU:\n",
    "            nonlin = 'leaky_relu'\n",
    "            a = .01\n",
    "        \n",
    "        # Init convolutional parameters\n",
    "        for layer in self.convs: \n",
    "            if layer.__class__.__name__ == 'Conv2d': nn.init.kaiming_normal_(layer.weight, a=a, nonlinearity=nonlin)\n",
    "        \n",
    "\n",
    "        # Init linear parameters\n",
    "        for i in (1, -1): nn.init.constant_(self.fc[i].bias, 0)\n",
    "        nn.init.kaiming_normal_(self.fc[1].weight, a=a, nonlinearity=nonlin)\n",
    "        nn.init.xavier_uniform_(self.fc[-1].weight)      \n",
    "        \n",
    "\n",
    "\n",
    "    def log_the_loss(self, item,epoch=False): # per avere una history della loss???\n",
    "        verbose=False\n",
    "        train = self.__getstate__()['training']\n",
    "        if verbose: print(train)\n",
    "        if epoch and train:\n",
    "            self.loss_dict['epoch'].append(item) ### get state of the model so you can ditch the validation parameter\n",
    "        elif not epoch and train:\n",
    "            self.loss_dict['batch'].append(item)\n",
    "        elif not train and epoch:\n",
    "            self.loss_dict['vepoch'].append(item)\n",
    "        elif not train and not epoch:\n",
    "            self.loss_dict['vbatch'].append(item)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fea87627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convs output size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "193it [00:12, 15.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.060939528955240276\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = GalaxyNet(nn.ReLU, initialization=True, mappy=False).to(device)\n",
    "loader = torch.load('/home/teobaldo/Uni/LCP-B/proj/fixed_torch/artifacts/24bc459d-e989-40b8-93d7-dbba3b98f0a5.pt', weights_only=True)['model_state_dict']\n",
    "model.load_state_dict(loader)\n",
    "model.eval()\n",
    "running_validation_loss = 0\n",
    "with torch.no_grad(): # deactivates gradient evaluation\n",
    "        for i, vdata in tqdm(enumerate(test_loader)):\n",
    "            inputs,labels, _ = vdata\n",
    "            inputs,labels= inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)#, activation=F.relu)\n",
    "            outputs = zk.mappy2D(outputs)\n",
    "            loss = nn.MSELoss()(outputs,labels)\n",
    "            RMSEloss = np.sqrt(loss.item())\n",
    "            running_validation_loss += RMSEloss\n",
    "            model.log_the_loss(RMSEloss,epoch=False)\n",
    "        mean_vloss=model.log_the_loss(running_validation_loss / len(test_loader),epoch=True)\n",
    "        print(f\"Test Loss: {mean_vloss}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d904556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyNet(nn.Module):\n",
    "    def __init__(self, activation, initialization=False, is_rgb=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        rgb = 3 if is_rgb else 1\n",
    "        input_size = 128\n",
    "        num_labels = 37\n",
    "        \n",
    "        self.loss_dict = {'batch' : [], 'epoch' : [], 'vbatch' : [], 'vepoch' : []}\n",
    "        self.activation = activation\n",
    "\n",
    "        \n",
    "        ## convolutional layers\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(rgb, 16, 3, bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 16, 3, bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, bias=False),\n",
    "            self.activation(),\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "\n",
    "        for layer in self.convs:\n",
    "            if layer.__class__.__name__ == 'Conv2d': input_size = zk.convool_size(input_size, 3, 1, 'same' if layer.padding == 'same' else 0)\n",
    "            elif layer.__class__.__name__ == 'MaxPool2d': input_size = zk.convool_size(input_size, 2, 2)\n",
    "\n",
    "        if input_size < 2: raise ValueError('You shrank too much dude.')\n",
    "        print(f'Convs output size: {input_size}')\n",
    "\n",
    "        input_linear = 128 * input_size * input_size\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_linear, 100),\n",
    "            self.activation(),\n",
    "            nn.Linear(100, num_labels)\n",
    "            )\n",
    "        \n",
    "        if initialization: self.init_weights()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        if self.activation == nn.ReLU:\n",
    "            nonlin = 'relu'\n",
    "            a = 0\n",
    "        elif self.activation == nn.LeakyReLU:\n",
    "            nonlin = 'leaky_relu'\n",
    "            a = .01\n",
    "        \n",
    "        # Init convolutional parameters\n",
    "        for layer in self.convs: \n",
    "            if layer.__class__.__name__ == 'Conv2d': nn.init.kaiming_normal_(layer.weight, a=a, nonlinearity=nonlin)\n",
    "        \n",
    "\n",
    "        # Init linear parameters\n",
    "        for i in (1, -1): nn.init.constant_(self.fc[i].bias, 0)\n",
    "        nn.init.kaiming_normal_(self.fc[1].weight, a=a, nonlinearity=nonlin)\n",
    "        nn.init.xavier_uniform_(self.fc[-1].weight)      \n",
    "        \n",
    "\n",
    "\n",
    "    def log_the_loss(self, item,epoch=False): # per avere una history della loss???\n",
    "        verbose=False\n",
    "        train = self.__getstate__()['training']\n",
    "        if verbose: print(train)\n",
    "        if epoch and train:\n",
    "            self.loss_dict['epoch'].append(item) ### get state of the model so you can ditch the validation parameter\n",
    "        elif not epoch and train:\n",
    "            self.loss_dict['batch'].append(item)\n",
    "        elif not train and epoch:\n",
    "            self.loss_dict['vepoch'].append(item)\n",
    "        elif not train and not epoch:\n",
    "            self.loss_dict['vbatch'].append(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e2e6f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "193it [00:07, 24.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.06623096033694796\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = pd.read_pickle('/home/teobaldo/Uni/LCP-B/proj/pickled/artifacts/6b74bc45-f2d8-43b1-b795-0da21c68bb2a').to(device)\n",
    "model.eval()\n",
    "running_validation_loss = 0\n",
    "with torch.no_grad(): # deactivates gradient evaluation\n",
    "        for i, vdata in tqdm(enumerate(test_loader)):\n",
    "            inputs,labels, _ = vdata\n",
    "            inputs,labels= inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)#, activation=F.relu)\n",
    "            outputs = zk.mappy2D(outputs)\n",
    "            loss = nn.MSELoss()(outputs,labels)\n",
    "            RMSEloss = np.sqrt(loss.item())\n",
    "            running_validation_loss += RMSEloss\n",
    "            model.log_the_loss(RMSEloss,epoch=False)\n",
    "        mean_vloss=model.log_the_loss(running_validation_loss / len(test_loader),epoch=True)\n",
    "        print(f\"Test Loss: {mean_vloss}\\n---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jungle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
